In these practical sessions, we implemented fundamental deep learning models for Computer Vision. To be more specific:

- In `1-ab: Intro to Neural Networks` section, we implemented a simple neural network from scratch, trained it on the MNIST dataset, and experimented with different values of various hyperparameters to have an insight into their effects on the network’s performance.
- In `1-cd: Convolutional Neural Networks` section, we implemented a simple CNN which has a style close to the AlexNet architecture and trained it on the CIFAR-10 dataset. Then, I tested various techniques to enhance the network’s learning process, including normalization, data augmentation, variants of SGD, dropout and batch normalization.
- In `1-e: Transformers` section, we implemented a naive ViT from scratch while experimenting with various hyperparameters such as embedding dimensions, patch sizes, and the number of transformer blocks. Then, I utilized established libraries like TIMM (PyTorch Image Models) to compare our implementation with pretrained models. This comparison provides insights into the best practices and potential improvements for enhancing big transformer's performance when being trained on small datasets.

For the subject of this practical session, please refer to [the website of the course](https://rdfia.github.io/) for more details.
