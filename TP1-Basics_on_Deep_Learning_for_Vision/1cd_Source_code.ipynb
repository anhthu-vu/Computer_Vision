{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgWb11WT-07r"
      },
      "source": [
        "<center><h1>1-cd: Convolutional Neural Networks (ConvNets)</h1></center>\n",
        "<b>Students:\n",
        "<pre>\n",
        "VU Anh Thu            \n",
        "LE Thi Minh Nguyet    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac5Js1iML3d_"
      },
      "outputs": [],
      "source": [
        "! wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-cd/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXEas44B52kC"
      },
      "outputs": [],
      "source": [
        "%run 'utils.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I7lgAEJvPCh"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from utils import *\n",
        "\n",
        "PRINT_INTERVAL = 200\n",
        "PATH=\"datasets\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Traing *from scratch* of the model**"
      ],
      "metadata": {
        "id": "DMVhTd-7IBsy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xVkUxvwy6a0"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "    # TODO\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(32, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(64, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10),\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output\n",
        "    # END TODO\n",
        "\n",
        "\n",
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "    train_dataset = datasets.CIFAR10(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "    val_dataset = datasets.CIFAR10(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def epoch(data, model, criterion, optimizer=None, cuda=False):\n",
        "    \"\"\"\n",
        "    Make a pass (called epoch in English) on the data `data` with the\n",
        "     model `model`. Evaluates `criterion` as loss.\n",
        "     If `optimizer` is given, perform a training epoch using\n",
        "     the given optimizer, otherwise, perform an evaluation epoch (no backward)\n",
        "     of the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # indicates whether the model is in eval or train mode (some layers behave differently in train and eval)\n",
        "    model.eval() if optimizer is None else model.train()\n",
        "\n",
        "    # objects to store metric averages\n",
        "    avg_loss = AverageMeter()\n",
        "    avg_top1_acc = AverageMeter()\n",
        "    avg_top5_acc = AverageMeter()\n",
        "    avg_batch_time = AverageMeter()\n",
        "    global loss_plot\n",
        "\n",
        "    # we iterate on the batches\n",
        "    tic = time.time()\n",
        "    for i, (input, target) in enumerate(data):\n",
        "\n",
        "        if cuda: # only with GPU, and not with CPU\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        # forward\n",
        "        output = model(input)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # backward if we are training\n",
        "        if optimizer:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # compute metrics\n",
        "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
        "        batch_time = time.time() - tic\n",
        "        tic = time.time()\n",
        "\n",
        "        # update\n",
        "        avg_loss.update(loss.item())\n",
        "        avg_top1_acc.update(prec1.item())\n",
        "        avg_top5_acc.update(prec5.item())\n",
        "        avg_batch_time.update(batch_time)\n",
        "        if optimizer:\n",
        "            loss_plot.update(avg_loss.val)\n",
        "        # print info\n",
        "        if i % PRINT_INTERVAL == 0:\n",
        "            print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n",
        "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n",
        "                  'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n",
        "                   \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n",
        "                   top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "            if optimizer:\n",
        "                loss_plot.plot()\n",
        "\n",
        "    # Print summary\n",
        "    print('\\n===============> Total time {batch_time:d}s\\t'\n",
        "          'Avg loss {loss.avg:.4f}\\t'\n",
        "          'Avg Prec@1 {top1.avg:5.2f} %\\t'\n",
        "          'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n",
        "           batch_time=int(avg_batch_time.sum), loss=avg_loss,\n",
        "           top1=avg_top1_acc, top5=avg_top5_acc))\n",
        "\n",
        "    return avg_top1_acc, avg_top5_acc, avg_loss\n",
        "\n",
        "\n",
        "def main(batch_size=128, lr=0.1, epochs=5, cuda=False):\n",
        "\n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "    if cuda: # only with GPU, and not with CPU\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Get the data\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "\n",
        "    # We iterate on the epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Train phase\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "        # Test phase\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        # plot\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "p_kLzNEQxWHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: Results improvements**\n",
        "We would like to note that from this point onward, all code has been either written by us or adapted from the provided code."
      ],
      "metadata": {
        "id": "mNR6GBS0IjZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Standardization of examples"
      ],
      "metadata": {
        "id": "pds60ZSbJmHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "    mean = [0.491, 0.482, 0.447] # Mean of each RGB channel over the whole train\n",
        "    std = [0.202, 0.199, 0.201] # Std of each RGB channel over the whole train\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ]))\n",
        "    val_dataset = datasets.CIFAR10(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "G1nFR3gHI7EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "ezNA8yAaKTBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other normalization methods (here we try ZCA whitening)\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each image.\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(PATH, train=True, download=True)\n",
        "    val_dataset = datasets.CIFAR10(PATH, train=False, download=True)\n",
        "\n",
        "    images = train_dataset.data # dim_images = B x H x W x C\n",
        "    B, H, W, C = images.shape\n",
        "    images = images.reshape(B, -1) # dim_images = B x (H*W*C)\n",
        "    mean = images.mean(axis=0)\n",
        "\n",
        "    cov = np.dot((images - mean).T, (images - mean))/(B-1) # dim_cov = (H*W*C) x (H*W*C)\n",
        "    u, s, _ = np.linalg.svd(cov)\n",
        "    epsilon = 1e-5\n",
        "    s = np.diag(1./np.sqrt(s+epsilon))\n",
        "\n",
        "    zca_matrix = np.dot(u, s.dot(u.T)) # dim_zca_matrix = (H*W*C) x (H*W*C)\n",
        "\n",
        "    images = np.dot(zca_matrix, images.T) # dim_images = (H*W*C) x B\n",
        "\n",
        "    train_dataset_zca = TensorDataset(torch.from_numpy(images.reshape(H, W, C, B).transpose(3, 2, 0, 1)).float(),\n",
        "                                      torch.tensor(train_dataset.targets))\n",
        "\n",
        "\n",
        "    val_images = val_dataset.data\n",
        "    B, H, W, C = val_images.shape\n",
        "    val_images = val_images.reshape(B, -1)\n",
        "    val_images = np.dot(zca_matrix, val_images.T)\n",
        "    val_dataset_zca = TensorDataset(torch.from_numpy(val_images.reshape(H, W, C, B).transpose(3, 2, 0, 1)).float(),\n",
        "                                    torch.tensor(val_dataset.targets))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset_zca,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset_zca,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "CB7iDk5Sp-8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=15, cuda=True)"
      ],
      "metadata": {
        "id": "86IvbwaWLmSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Increase in the number of training examples by *data increase*"
      ],
      "metadata": {
        "id": "NG15_5V5Tnaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(32, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(64, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0, ceil_mode=True), # add ceil_mode=True to counter smaller image size\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10),\n",
        "\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output"
      ],
      "metadata": {
        "id": "VUiENfroWzCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomCrop(28),           # Random crop of size 28x28\n",
        "            transforms.RandomHorizontalFlip(),   # Random horizontal flip with p=0.5\n",
        "        ]))\n",
        "    val_dataset = datasets.CIFAR10(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.CenterCrop(28), # Center crop of size 28x28\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "    return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "A6COuowyTmls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=45, cuda=True)"
      ],
      "metadata": {
        "id": "1MiDF_-OWs77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other data augumentation methods\n",
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.RandomRotation(10), # Random rotation by up to 10 degrees\n",
        "        ]))\n",
        "    val_dataset = datasets.CIFAR10(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "Rioyz1eprOda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=30, cuda=True)"
      ],
      "metadata": {
        "id": "rIvEfzUb1Iyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3. Variants on the optimization algorithm"
      ],
      "metadata": {
        "id": "NPJFaQFRsuBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler\n",
        "\n",
        "def get_dataset(batch_size, cuda=False):\n",
        "    \"\"\"\n",
        "    This function loads the dataset and performs transformations on each\n",
        "    image (listed in `transform = ...`).\n",
        "    \"\"\"\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(PATH, train=True, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ]))\n",
        "    val_dataset = datasets.CIFAR10(PATH, train=False, download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ]))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                        batch_size=batch_size, shuffle=True, pin_memory=cuda, num_workers=2)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
        "                        batch_size=batch_size, shuffle=False, pin_memory=cuda, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "jPBHG01ws0XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, lr=0.1, epochs=5, cuda=False):\n",
        "\n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "    lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95) # Create the scheduler\n",
        "    if cuda: # only with GPU, and not with CPU\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Get the data\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "    top1_acc_train = []\n",
        "    avg_top5_acc_train = []\n",
        "    loss_train = []\n",
        "\n",
        "    # We iterate on the epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Train phase\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "        lr_sched.step() # Modify the learning rate after each epoch\n",
        "\n",
        "        # Test phase\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)"
      ],
      "metadata": {
        "id": "JYkUwzNttjSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "uFf7Uzf-_1O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Other variants of learning rate planning strategy and SGD\n",
        "# Step decay\n",
        "def main(batch_size=128, lr=0.1, epochs=5, cuda=False):\n",
        "\n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "    lr_sched = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.1) # Decays the learning rate by 0.1 every 10 epochs\n",
        "    if cuda: # only with GPU, and not with CPU\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Get the data\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "    top1_acc_train = []\n",
        "    avg_top5_acc_train = []\n",
        "    loss_train = []\n",
        "\n",
        "    # We iterate on the epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Train phase\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "        lr_sched.step() # Modify the learning rate after each epoch\n",
        "\n",
        "        # Test phase\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)"
      ],
      "metadata": {
        "id": "HjWwwx2Ju_PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "u6PV-c-BzXCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam\n",
        "def main(batch_size=128, lr=0.001, epochs=5, cuda=False):\n",
        "\n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "\n",
        "    if cuda: # only with GPU, and not with CPU\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Get the data\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "    top1_acc_train = []\n",
        "    avg_top5_acc_train = []\n",
        "    loss_train = []\n",
        "\n",
        "    # We iterate on the epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Train phase\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "\n",
        "        # Test phase\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)"
      ],
      "metadata": {
        "id": "nmZOu42SC4T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "FXXPehXYD_h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Regularization of the network by *dropout*"
      ],
      "metadata": {
        "id": "q5j1a8SECUy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(32, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(64, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.65), # Dropout layer\n",
        "            nn.Linear(1000, 10),\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output"
      ],
      "metadata": {
        "id": "--vO9yVLVvfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, lr=0.1, epochs=5, cuda=False):\n",
        "\n",
        "    # define model, loss, optim\n",
        "    model = ConvNet()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "\n",
        "    if cuda: # only with GPU, and not with CPU\n",
        "        cudnn.benchmark = True\n",
        "        model = model.cuda()\n",
        "        criterion = criterion.cuda()\n",
        "\n",
        "    # Get the data\n",
        "    train, test = get_dataset(batch_size, cuda)\n",
        "\n",
        "    # init plots\n",
        "    plot = AccLossPlot()\n",
        "    global loss_plot\n",
        "    loss_plot = TrainLossPlot()\n",
        "\n",
        "    # We iterate on the epochs\n",
        "    for i in range(epochs):\n",
        "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
        "        # Train phase\n",
        "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer, cuda)\n",
        "        # Test phase\n",
        "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion, cuda=cuda)\n",
        "        # plot\n",
        "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)"
      ],
      "metadata": {
        "id": "UUr9yKH2CwDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "Pa4qMLanVwP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Use of *batch normalization*"
      ],
      "metadata": {
        "id": "BxIdQ7hdYSAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    This class defines the structure of the neural network\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # We first define the convolution and pooling layers as a features extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32), # BatchNorm layer\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(32, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64), # # BatchNorm layer\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "            nn.Conv2d(64, 64, (5, 5), stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64), # # BatchNorm layer\n",
        "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
        "        )\n",
        "        # We then define fully connected layers as a classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 1000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1000, 10),\n",
        "        )\n",
        "\n",
        "    # Method called when we apply the network to an input batch\n",
        "    def forward(self, input):\n",
        "        bsize = input.size(0) # batch size\n",
        "        output = self.features(input) # output of the conv layers\n",
        "        output = output.view(bsize, -1) # we flatten the 2D feature maps into one 1D vector for each input\n",
        "        output = self.classifier(output) # we compute the output of the fc layers\n",
        "        return output"
      ],
      "metadata": {
        "id": "Zn9SIzz9YQ1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(epochs=25, cuda=True)"
      ],
      "metadata": {
        "id": "ZibbUxoEZBgz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}