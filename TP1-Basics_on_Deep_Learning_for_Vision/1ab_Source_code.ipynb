{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbzBJ1m9FBBb"
      },
      "source": [
        "<center><h1>1-ab: Introduction to Neural Networks</h1></center>\n",
        "<b>Students:\n",
        "<pre>\n",
        "VU Anh Thu            \n",
        "LE Thi Minh Nguyet    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfnKy8NB8J5e"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/rdfia/rdfia.github.io/raw/master/data/2-ab.zip\n",
        "!unzip -j 2-ab.zip\n",
        "!wget https://github.com/rdfia/rdfia.github.io/raw/master/code/2-ab/utils-data.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vQ_LLdx8J5b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%run 'utils-data.py'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48x_ha7f8J5i"
      },
      "source": [
        "# Part 1 : Forward and backward passes \"by hands\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtizX1JV8J5n"
      },
      "outputs": [],
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "\n",
        "    # TODO\n",
        "    params[\"Wh\"] = 0.3*torch.randn((nh, nx))\n",
        "    params[\"Wy\"] = 0.3*torch.randn((ny, nh))\n",
        "    params[\"bh\"] = 0.3*torch.randn((1, nh))\n",
        "    params[\"by\"] = 0.3*torch.randn((1, ny))\n",
        "    # END TODO\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk-N_Ny67yo-"
      },
      "outputs": [],
      "source": [
        "def forward(params, X):\n",
        "    \"\"\"\n",
        "    params: dictionnary\n",
        "    X: (n_batch, dimension)\n",
        "    \"\"\"\n",
        "    outputs = {}\n",
        "\n",
        "    # TODO\n",
        "    outputs[\"X\"] = X\n",
        "    outputs[\"htilde\"] = torch.mm(X, params['Wh'].t()) + params[\"bh\"]\n",
        "    outputs[\"h\"] = torch.tanh(outputs[\"htilde\"])\n",
        "    outputs[\"ytilde\"] = torch.mm(outputs[\"h\"], params['Wy'].t()) + params[\"by\"]\n",
        "    exp_ytilde = torch.exp(outputs[\"ytilde\"])\n",
        "    outputs[\"yhat\"] = exp_ytilde/torch.sum(exp_ytilde, dim=1, keepdim=True)\n",
        "    # END TODO\n",
        "\n",
        "    return outputs['yhat'], outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uB0A2b28NZK"
      },
      "outputs": [],
      "source": [
        "def loss_accuracy(Yhat, Y):\n",
        "    # TODO\n",
        "    L = -torch.sum(Y*torch.log(Yhat))/Y.shape[0]\n",
        "\n",
        "    _, indsY = torch.max(Y, 1)\n",
        "    _, indsYhat = torch.max(Yhat, 1)\n",
        "    acc = torch.sum(indsY == indsYhat).float()/Y.shape[0]\n",
        "    # END TODO\n",
        "\n",
        "    return L, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWJjdiFe8qi5"
      },
      "outputs": [],
      "source": [
        "def backward(params, outputs, Y):\n",
        "    grads = {}\n",
        "\n",
        "    # TODO\n",
        "    grad_ytilde = outputs['yhat'] - Y\n",
        "    grads[\"Wy\"] = torch.mm(grad_ytilde.t(), outputs['h'])\n",
        "    grad_htilde = grad_ytilde.mm(params[\"Wy\"]) * (1-outputs[\"h\"]**2)\n",
        "    grads[\"Wh\"] = torch.mm(grad_htilde.t(), outputs[\"X\"])\n",
        "    grads[\"by\"] = torch.sum(grad_ytilde, dim=0, keepdim=True)\n",
        "    grads[\"bh\"] = torch.sum(grad_htilde, dim=0, keepdim=True)\n",
        "    # END TODO\n",
        "\n",
        "    return grads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAnsISsW9CnH"
      },
      "outputs": [],
      "source": [
        "def sgd(params, grads, eta):\n",
        "    # update the params values\n",
        "    # TODO\n",
        "    params[\"Wh\"] = params[\"Wh\"] - eta*grads[\"Wh\"]\n",
        "    params[\"Wy\"] = params[\"Wy\"] - eta*grads[\"Wy\"]\n",
        "    params[\"bh\"] = params[\"bh\"] - eta*grads[\"bh\"]\n",
        "    params[\"by\"] = params[\"by\"] - eta*grads[\"by\"]\n",
        "    # END TODO\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifuW5UFA3DZ"
      },
      "source": [
        "## Global learning procedure \"by hands\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RSw6bd0-qUe"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Optimization algorithm on the batch (X,Y)\n",
        "        # TODO\n",
        "        _, outputs = forward(params, X)\n",
        "        grads = backward(params, outputs, Y)\n",
        "        params = sgd(params, grads, eta)\n",
        "        # END TODO\n",
        "\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain*100, Ltrain, acctest*100, Ltest)\n",
        "    data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHHH5PL8J54"
      },
      "source": [
        "# Part 2 : Simplification of the backward pass with `torch.autograd`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7G4q5zP0CEvB"
      },
      "outputs": [],
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "\n",
        "    # activate autograd on the network weights\n",
        "    # TODO\n",
        "    params[\"Wh\"] = 0.3*torch.randn((nh, nx))\n",
        "    params[\"Wy\"] = 0.3*torch.randn((ny, nh))\n",
        "    params[\"bh\"] = 0.3*torch.randn((1, nh))\n",
        "    params[\"by\"] = 0.3*torch.randn((1, ny))\n",
        "\n",
        "    params[\"Wh\"].requires_grad = True\n",
        "    params[\"Wy\"].requires_grad = True\n",
        "    params[\"bh\"].requires_grad = True\n",
        "    params[\"by\"].requires_grad = True\n",
        "    # END TODO\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0tSjpKCyVB"
      },
      "source": [
        "The function `forward` remains unchanged from previous part.\n",
        "\n",
        "The function `backward` is no longer used because of \"autograd\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA4ycHlfBzCK"
      },
      "outputs": [],
      "source": [
        "def sgd(params, eta):\n",
        "    # update the network weights and reset to zero the gradient accumulators\n",
        "    # TODO\n",
        "    with torch.no_grad():\n",
        "      params[\"Wh\"] -= eta*params[\"Wh\"].grad\n",
        "      params[\"Wy\"] -= eta*params[\"Wy\"].grad\n",
        "      params[\"bh\"] -= eta*params[\"bh\"].grad\n",
        "      params[\"by\"] -= eta*params[\"by\"].grad\n",
        "\n",
        "      params[\"Wh\"].grad.zero_()\n",
        "      params[\"Wy\"].grad.zero_()\n",
        "      params[\"bh\"].grad.zero_()\n",
        "      params[\"by\"].grad.zero_()\n",
        "    # END TODO\n",
        "\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjgcmgQpDfOb"
      },
      "source": [
        "## Global learning procedure with autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8p5oR3EqDea-"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Optimization algorithm on the batch (X,Y)\n",
        "        # TODO\n",
        "        Yhat, _ = forward(params, X)\n",
        "        L, _ = loss_accuracy(Yhat, Y)\n",
        "        L.backward()\n",
        "        params = sgd(params, eta)\n",
        "        # END TODO\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain*100, Ltrain, acctest*100, Ltest)\n",
        "    # detach() is used to remove the predictions from the computational graph in autograd\n",
        "    data.plot_data_with_grid(Ygrid.detach(), title)\n",
        "\n",
        "    curves[0].append(acctrain.detach())\n",
        "    curves[1].append(acctest.detach())\n",
        "    curves[2].append(Ltrain.detach())\n",
        "    curves[3].append(Ltest.detach())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FV1iss68J6H"
      },
      "source": [
        "# Part 3 : Simplification of the forward pass with `torch.nn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6T5Uq7JEl47"
      },
      "source": [
        "`init_params` and `forward` are replaced by the `init_model` function which defines the network architecture and the loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-h4r-FH8J6I"
      },
      "outputs": [],
      "source": [
        "def init_model(nx, nh, ny):\n",
        "    # TODO\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(nx, nh),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(nh, ny),\n",
        "    )\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    # END TODO\n",
        "\n",
        "    return model, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geE_TI96FXnl"
      },
      "outputs": [],
      "source": [
        "def loss_accuracy(loss, Ytilde, Y):\n",
        "    # TODO\n",
        "    _, indsY = torch.max(Y, 1)\n",
        "    Yhat = torch.nn.Softmax(dim=1)(Ytilde)\n",
        "    _, indsYhat = torch.max(Yhat, 1)\n",
        "\n",
        "    L = loss(Ytilde, indsY)\n",
        "    acc = torch.sum(indsY == indsYhat).float()/Y.shape[0]\n",
        "    # END TODO\n",
        "\n",
        "    return L, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e93bvFiYGKnA"
      },
      "outputs": [],
      "source": [
        "def sgd(model, eta):\n",
        "    # update the network weights and reset to zero the gradient accumulators\n",
        "    # TODO\n",
        "    with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "        param -= eta*param.grad\n",
        "      model.zero_grad()\n",
        "    # END TODO\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOxBMmD4Gxtp"
      },
      "source": [
        "## Global learning procedure with autograd and `torch.nn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMBmCNvHCLn"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "model, loss = init_model(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # optimization algorithm on the batch (X,Y)\n",
        "        # TODO\n",
        "        Ytilde = model(X)\n",
        "        L, _ = loss_accuracy(loss, Ytilde, Y)\n",
        "        L.backward()\n",
        "        model = sgd(model, eta)\n",
        "        # END TODO\n",
        "\n",
        "    Ytilde_train = model(data.Xtrain)\n",
        "    Ytilde_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Ytilde_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Ytilde_test, data.Ytest)\n",
        "    Ygrid = model(data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain*100, Ltrain, acctest*100, Ltest)\n",
        "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
        "\n",
        "    curves[0].append(acctrain.detach())\n",
        "    curves[1].append(acctest.detach())\n",
        "    curves[2].append(Ltrain.detach())\n",
        "    curves[3].append(Ltest.detach())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoFSrQNsJCnz"
      },
      "source": [
        "# Part 4 : Simplification of the SGD with `torch.optim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8WtN9loJPqP"
      },
      "outputs": [],
      "source": [
        "def init_model(nx, nh, ny, eta):\n",
        "    # TODO\n",
        "    model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(nx, nh),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(nh, ny),\n",
        "    )\n",
        "\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=eta)\n",
        "    # END TODO\n",
        "\n",
        "    return model, loss, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-0rRzPJYDd"
      },
      "source": [
        "The `sgd` function is replaced by calling the `optim.zero_grad()` before the backward and `optim.step()` after."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q82hCupvJxvV"
      },
      "source": [
        "## Algorithme global d'apprentissage (avec autograd, les couches `torch.nn` et `torch.optim`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9h9nINKJ1LU"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "model, loss, optim = init_model(nx, nh, ny, eta)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # optimization algorithm on the batch (X,Y)\n",
        "        # TODO\n",
        "        optim.zero_grad()\n",
        "        Ytilde = model(X)\n",
        "        L, _ = loss_accuracy(loss, Ytilde, Y)\n",
        "        L.backward()\n",
        "        optim.step()\n",
        "        # END TODO\n",
        "\n",
        "    Ytilde_train = model(data.Xtrain)\n",
        "    Ytilde_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Ytilde_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Ytilde_test, data.Ytest)\n",
        "    Ygrid = model(data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain*100, Ltrain, acctest*100, Ltest)\n",
        "    data.plot_data_with_grid(torch.nn.Softmax(dim=1)(Ygrid.detach()), title)\n",
        "\n",
        "    curves[0].append(acctrain.detach())\n",
        "    curves[1].append(acctest.detach())\n",
        "    curves[2].append(Ltrain.detach())\n",
        "    curves[3].append(Ltest.detach())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extension: Experiments on different values of different hyperparameters**"
      ],
      "metadata": {
        "id": "5vT7zGJYPsLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Different learning rates\n",
        "\n",
        "data = CirclesData()\n",
        "N = data.Xtrain.shape[0]\n",
        "nx = data.Xtrain.shape[1]\n",
        "ny = data.Ytrain.shape[1]\n",
        "\n",
        "batch_size = 10\n",
        "hidden_units = 10\n",
        "learning_rates = [0.01, 0.1, 1]\n",
        "\n",
        "results_lr = {}\n",
        "\n",
        "for eta in learning_rates:\n",
        "    model, loss, optim = init_model(nx, hidden_units, ny, eta)\n",
        "    curves = [[], [], [], []]\n",
        "\n",
        "    for iteration in range(150):\n",
        "        perm = np.random.permutation(N)\n",
        "        Xtrain = data.Xtrain[perm, :]\n",
        "        Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "        for j in range(N // batch_size):\n",
        "            indsBatch = range(j * batch_size, (j + 1) * batch_size)\n",
        "            X = Xtrain[indsBatch, :]\n",
        "            Y = Ytrain[indsBatch, :]\n",
        "\n",
        "            optim.zero_grad()\n",
        "            Ytilde = model(X)\n",
        "            L, _ = loss_accuracy(loss, Ytilde, Y)\n",
        "            L.backward()\n",
        "            optim.step()\n",
        "\n",
        "        # Evaluate on train and test data\n",
        "        Ytilde_train = model(data.Xtrain)\n",
        "        Ytilde_test = model(data.Xtest)\n",
        "        Ltrain, acctrain = loss_accuracy(loss, Ytilde_train, data.Ytrain)\n",
        "        Ltest, acctest = loss_accuracy(loss, Ytilde_test, data.Ytest)\n",
        "\n",
        "        # Save accuracy and loss\n",
        "        curves[0].append(acctrain.detach())  # Train accuracy\n",
        "        curves[1].append(acctest.detach())   # Test accuracy\n",
        "        curves[2].append(Ltrain.detach())  # Train loss\n",
        "        curves[3].append(Ltest.detach())   # Test loss\n",
        "\n",
        "    results_lr[eta] = curves\n",
        "\n",
        "# Plot learning curves for different learning rates\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "for eta, curves in results_lr.items():\n",
        "    axs[0].plot(curves[0], label=f\"Train Acc, LR={eta}\")\n",
        "    axs[0].plot(curves[1], label=f\"Test Acc, LR={eta}\")\n",
        "    axs[1].plot(curves[2], label=f\"Train Loss, LR={eta}\")\n",
        "    axs[1].plot(curves[3], label=f\"Test Loss, LR={eta}\")\n",
        "\n",
        "axs[0].set_title(\"Accuracy vs. Epochs (Learning Rate)\")\n",
        "axs[0].legend()\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "axs[1].set_title(\"Loss vs. Epochs (Learning Rate)\")\n",
        "axs[1].legend()\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6rC8eHFPrWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different batch sizes\n",
        "\n",
        "data = CirclesData()\n",
        "N = data.Xtrain.shape[0]\n",
        "nx = data.Xtrain.shape[1]\n",
        "ny = data.Ytrain.shape[1]\n",
        "\n",
        "learning_rate = 0.03\n",
        "hidden_units = 10\n",
        "batch_sizes = [1, 30, 100]\n",
        "\n",
        "results_batch = {}\n",
        "\n",
        "for Nbatch in batch_sizes:\n",
        "    model, loss, optim = init_model(nx, hidden_units, ny, learning_rate)\n",
        "    curves = [[], [], [], []]\n",
        "\n",
        "    for iteration in range(150):\n",
        "        perm = np.random.permutation(N)\n",
        "        Xtrain = data.Xtrain[perm, :]\n",
        "        Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "        for j in range(N // Nbatch):\n",
        "            indsBatch = range(j * Nbatch, (j + 1) * Nbatch)\n",
        "            X = Xtrain[indsBatch, :]\n",
        "            Y = Ytrain[indsBatch, :]\n",
        "\n",
        "            Ytilde = model(X)\n",
        "            L, _ = loss_accuracy(loss, Ytilde, Y)\n",
        "            L.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "        # Evaluate on train and test data\n",
        "        Ytilde_train = model(data.Xtrain)\n",
        "        Ytilde_test = model(data.Xtest)\n",
        "        Ltrain, acctrain = loss_accuracy(loss, Ytilde_train, data.Ytrain)\n",
        "        Ltest, acctest = loss_accuracy(loss, Ytilde_test, data.Ytest)\n",
        "\n",
        "        # Save accuracy and loss\n",
        "        curves[0].append(acctrain.detach())  # Train accuracy\n",
        "        curves[1].append(acctest.detach())   # Test accuracy\n",
        "        curves[2].append(Ltrain.detach())  # Train loss\n",
        "        curves[3].append(Ltest.detach())   # Test loss\n",
        "\n",
        "    results_batch[Nbatch] = curves\n",
        "\n",
        "# Plot learning curves for different batch sizes\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "for Nbatch, curves in results_batch.items():\n",
        "    axs[0].plot(curves[0], label=f\"Train Acc, Batch={Nbatch}\")\n",
        "    axs[0].plot(curves[1], label=f\"Test Acc, Batch={Nbatch}\")\n",
        "    axs[1].plot(curves[2], label=f\"Train Loss, Batch={Nbatch}\")\n",
        "    axs[1].plot(curves[3], label=f\"Test Loss, Batch={Nbatch}\")\n",
        "\n",
        "axs[0].set_title(\"Accuracy vs. Epochs (Batch Size)\")\n",
        "axs[0].legend()\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "axs[1].set_title(\"Loss vs. Epochs (Batch Size)\")\n",
        "axs[1].legend()\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vwVSVt5ZXlDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Different hidden units\n",
        "\n",
        "data = CirclesData()\n",
        "N = data.Xtrain.shape[0]\n",
        "nx = data.Xtrain.shape[1]\n",
        "ny = data.Ytrain.shape[1]\n",
        "\n",
        "learning_rate = 0.03\n",
        "batch_size = 10\n",
        "hidden_units_list = [1, 30, 100]\n",
        "\n",
        "results_hidden = {}\n",
        "\n",
        "for nh in hidden_units_list:\n",
        "    model, loss, optim = init_model(nx, nh, ny, learning_rate)\n",
        "    curves = [[], [], [], []]\n",
        "\n",
        "    for iteration in range(150):\n",
        "        perm = np.random.permutation(N)\n",
        "        Xtrain = data.Xtrain[perm, :]\n",
        "        Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "        for j in range(N // batch_size):\n",
        "            indsBatch = range(j * batch_size, (j + 1) * batch_size)\n",
        "            X = Xtrain[indsBatch, :]\n",
        "            Y = Ytrain[indsBatch, :]\n",
        "\n",
        "            Ytilde = model(X)\n",
        "            L, _ = loss_accuracy(loss, Ytilde, Y)\n",
        "            L.backward()\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "        # Evaluate on train and test data\n",
        "        Ytilde_train = model(data.Xtrain)\n",
        "        Ytilde_test = model(data.Xtest)\n",
        "        Ltrain, acctrain = loss_accuracy(loss, Ytilde_train, data.Ytrain)\n",
        "        Ltest, acctest = loss_accuracy(loss, Ytilde_test, data.Ytest)\n",
        "\n",
        "        # Save accuracy and loss\n",
        "        curves[0].append(acctrain.detach())  # Train accuracy\n",
        "        curves[1].append(acctest.detach())   # Test accuracy\n",
        "        curves[2].append(Ltrain.detach())  # Train loss\n",
        "        curves[3].append(Ltest.detach())   # Test loss\n",
        "\n",
        "    results_hidden[nh] = curves\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "for nh, curves in results_hidden.items():\n",
        "    axs[0].plot(curves[0], label=f\"Train Acc, Hidden={nh}\")\n",
        "    axs[0].plot(curves[1], label=f\"Test Acc, Hidden={nh}\")\n",
        "    axs[1].plot(curves[2], label=f\"Train Loss, Hidden={nh}\")\n",
        "    axs[1].plot(curves[3], label=f\"Test Loss, Hidden={nh}\")\n",
        "\n",
        "axs[0].set_title(\"Accuracy vs. Epochs (Hidden Units)\")\n",
        "axs[0].legend()\n",
        "axs[0].set_xlabel(\"Epochs\")\n",
        "axs[0].set_ylabel(\"Accuracy\")\n",
        "\n",
        "axs[1].set_title(\"Loss vs. Epochs (Hidden Units)\")\n",
        "axs[1].legend()\n",
        "axs[1].set_xlabel(\"Epochs\")\n",
        "axs[1].set_ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H6Ek9UXXYroz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1s4JuOSaZ3"
      },
      "source": [
        "# Part 5 : MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jly9C4FCSzLP"
      },
      "source": [
        "Apply the code from previous part code to the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osrFoEr_Syi7"
      },
      "outputs": [],
      "source": [
        "# init\n",
        "data = MNISTData()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 100\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 100\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "# TODO\n",
        "model, loss, optim = init_model(nx, nh, ny, eta)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        optim.zero_grad()\n",
        "        Ytilde = model(X)\n",
        "        L, _ = loss_accuracy(loss, Ytilde, Y)\n",
        "        L.backward()\n",
        "        optim.step()\n",
        "\n",
        "    Ytilde_train = model(data.Xtrain)\n",
        "    Ytilde_test = model(data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(loss, Ytilde_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(loss, Ytilde_test, data.Ytest)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain*100, Ltrain, acctest*100, Ltest)\n",
        "    print (title)\n",
        "\n",
        "    curves[0].append(acctrain.detach())\n",
        "    curves[1].append(acctest.detach())\n",
        "    curves[2].append(Ltrain.detach())\n",
        "    curves[3].append(Ltest.detach())\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# END TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRoiGbhvmSLO"
      },
      "source": [
        "# Part 6: Bonus: SVM\n",
        "\n",
        "\n",
        "Train a SVM model on the Circles dataset.\n",
        "\n",
        "Ideas :\n",
        "- First try a linear SVM (sklearn.svm.LinearSVC dans scikit-learn). Does it work well ? Why ?\n",
        "- Then try more complex kernels (sklearn.svm.SVC). Which one is the best ? why ?\n",
        "- Does the parameter C of regularization have an impact? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWeW8siymR3g"
      },
      "outputs": [],
      "source": [
        "# data\n",
        "data = CirclesData()\n",
        "Xtrain = data.Xtrain.numpy()\n",
        "Ytrain = data.Ytrain[:, 0].numpy()\n",
        "\n",
        "Xgrid = data.Xgrid.numpy()\n",
        "\n",
        "Xtest = data.Xtest.numpy()\n",
        "Ytest = data.Ytest[:, 0].numpy()\n",
        "\n",
        "def plot_svm_predictions(data, predictions):\n",
        "      plt.figure(2)\n",
        "      plt.clf()\n",
        "      plt.imshow(np.reshape(predictions, (40,40)))\n",
        "      plt.plot(data._Xtrain[data._Ytrain[:,0] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,0] == 1,1]*10+20, 'bo', label=\"Train\")\n",
        "      plt.plot(data._Xtrain[data._Ytrain[:,1] == 1,0]*10+20, data._Xtrain[data._Ytrain[:,1] == 1,1]*10+20, 'ro')\n",
        "      plt.plot(data._Xtest[data._Ytest[:,0] == 1,0]*10+20, data._Xtest[data._Ytest[:,0] == 1,1]*10+20, 'b+', label=\"Test\")\n",
        "      plt.plot(data._Xtest[data._Ytest[:,1] == 1,0]*10+20, data._Xtest[data._Ytest[:,1] == 1,1]*10+20, 'r+')\n",
        "      plt.xlim(0,39)\n",
        "      plt.ylim(0,39)\n",
        "      plt.clim(0.3,0.7)\n",
        "      plt.draw()\n",
        "      plt.pause(1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Extension: Experiments on different kernels and diffirent values of $C$**"
      ],
      "metadata": {
        "id": "GVgJeTRSDrb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Linear SVM**"
      ],
      "metadata": {
        "id": "iR42BIfPeom4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1xcE6zbmXU1"
      },
      "outputs": [],
      "source": [
        "import sklearn.svm\n",
        "\n",
        "svm = sklearn.svm.LinearSVC()\n",
        "svm.fit(Xtrain, Ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgLl7B_3mbOs"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "\n",
        "Ytest_pred = svm.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}%\")\n",
        "Ygrid_pred = svm.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **More complex kernels**"
      ],
      "metadata": {
        "id": "09fvQqOTewVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gaussian kernel (rbf)\n",
        "svm = sklearn.svm.SVC(kernel='rbf')\n",
        "svm.fit(Xtrain, Ytrain)"
      ],
      "metadata": {
        "id": "h6vqFkeBevhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "Ytest_pred = svm.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}%\")\n",
        "Ygrid_pred = svm.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ],
      "metadata": {
        "id": "JmrTL1s_fLvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Polynomial kernel\n",
        "svm = sklearn.svm.SVC(kernel='poly')\n",
        "svm.fit(Xtrain, Ytrain)"
      ],
      "metadata": {
        "id": "nsGKF5_qfPCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "Ytest_pred = svm.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}%\")\n",
        "Ygrid_pred = svm.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ],
      "metadata": {
        "id": "KkFoht8rfXHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid kernel\n",
        "svm = sklearn.svm.SVC(kernel='sigmoid')\n",
        "svm.fit(Xtrain, Ytrain)"
      ],
      "metadata": {
        "id": "80gp9ajIfX9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "Ytest_pred = svm.predict(Xtest)\n",
        "accuracy = np.sum(Ytest == Ytest_pred) / len(Ytest)\n",
        "print(f\"Accuracy : {100 * accuracy:.2f}%\")\n",
        "Ygrid_pred = svm.predict(Xgrid)\n",
        "plot_svm_predictions(data, Ygrid_pred)"
      ],
      "metadata": {
        "id": "4j1bp9JBffN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Impacts of C**"
      ],
      "metadata": {
        "id": "7y8hgw6jjpwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "C = np.linspace(0.01, 2., 200)\n",
        "acc_train = []\n",
        "acc_test = []\n",
        "\n",
        "for c in C:\n",
        "  svm = sklearn.svm.SVC(kernel='rbf', C=c)\n",
        "  svm.fit(Xtrain, Ytrain)\n",
        "  Ytrain_pred = svm.predict(Xtrain)\n",
        "  Ytest_pred = svm.predict(Xtest)\n",
        "  acc_train.append(100*np.sum(Ytrain == Ytrain_pred) / len(Ytrain))\n",
        "  acc_test.append(100*np.sum(Ytrain == Ytest_pred) / len(Ytest))"
      ],
      "metadata": {
        "id": "1coP0TFjjoja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curves\n",
        "plt.plot(C, acc_train, label=\"Train\")\n",
        "plt.plot(C, acc_test, label=\"Test\")\n",
        "plt.xlabel(\"C\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J50uRBAqkWGy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}