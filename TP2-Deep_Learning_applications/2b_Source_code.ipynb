{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"f5J96zex-R3G"},"source":["# 2-b: Visualizing Neural Networks\n","\n","<b>Students:\n","<pre>\n","VU Anh Thu            <font color=blue>21322736</font>\n","LE Thi Minh Nguyet    <font color=blue>21401438</font>\n","\n"]},{"cell_type":"code","metadata":{"id":"Du3JMqfh-R3N"},"source":["import torch\n","import torchvision\n","from torchvision import transforms\n","import random\n","import numpy as np\n","from scipy.ndimage import gaussian_filter1d\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import os\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'viridis'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UO1mXODA-R3Z"},"source":["## Functions and useful variables"]},{"cell_type":"code","metadata":{"id":"92UaaZxw-R3c"},"source":["SQUEEZENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","SQUEEZENET_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","\n","def preprocess(img, size=224):\n","    transform = transforms.Compose([\n","        transforms.Resize(size),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=SQUEEZENET_MEAN.tolist(),\n","                    std=SQUEEZENET_STD.tolist()),\n","        transforms.Lambda(lambda x: x[None]),  # add one dimension => dim = B x C x H x W\n","    ])\n","    return transform(img)\n","\n","def deprocess(img, should_rescale=True):\n","    transform = transforms.Compose([\n","        transforms.Lambda(lambda x: x[0]),\n","        transforms.Normalize(mean=[0, 0, 0], std=(1.0 / SQUEEZENET_STD).tolist()),\n","        transforms.Normalize(mean=(-SQUEEZENET_MEAN).tolist(), std=[1, 1, 1]),\n","        transforms.Lambda(rescale) if should_rescale else transforms.Lambda(lambda x: x),\n","        transforms.ToPILImage(),\n","    ])\n","    return transform(img)\n","\n","def rescale(x):\n","    low, high = x.min(), x.max()\n","    x_rescaled = (x - low) / (high - low)\n","    return x_rescaled\n","\n","def blur_image(X, sigma=1):\n","    X_np = X.cpu().clone().detach().numpy() # dim = B x C x H x W\n","    X_np = gaussian_filter1d(X_np, sigma, axis=2)\n","    X_np = gaussian_filter1d(X_np, sigma, axis=3)\n","    X.copy_(torch.Tensor(X_np).type_as(X))\n","    return X\n","\n","def jitter(X, ox, oy):\n","    \"\"\"\n","    Helper function to randomly jitter an image.\n","\n","    Inputs\n","    - X: PyTorch Tensor of shape (N, C, H, W)\n","    - ox, oy: Integers giving number of pixels to jitter along W and H axes\n","\n","    Returns: A new PyTorch Tensor of shape (N, C, H, W)\n","    \"\"\"\n","    if ox != 0:\n","        left = X[:, :, :, :-ox]\n","        right = X[:, :, :, -ox:]\n","        X = torch.cat([right, left], dim=3)\n","    if oy != 0:\n","        top = X[:, :, :-oy]\n","        bottom = X[:, :, -oy:]\n","        X = torch.cat([bottom, top], dim=2)\n","    return X"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxYQoVhx-R3k"},"source":["## Load the model\n","\n","For this TME, we will use the Squeezenet model which is a light model pre-trained on ImageNet. This model will be frozen: the goal is not to modify or train the weights but to study them."]},{"cell_type":"code","metadata":{"id":"QPwWLUAD-R3n"},"source":["# Load the model\n","model = torchvision.models.squeezenet1_1(weights='SqueezeNet1_1_Weights.DEFAULT')\n","\n","# Model in test mode\n","model.eval()\n","\n","# Freeze the weights\n","for param in model.parameters():\n","    param.requires_grad = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lF3hHqWr-R3w"},"source":["## Load example images\n","\n","This will fill the variables `X, y, class_names` with 25 examples from the validation set of ImageNet. `X` containes the images, `y` the class index of each image, and `class_names` a dictionary giving the class name from its index."]},{"cell_type":"code","metadata":{"id":"Lkg4v3wq_v58"},"source":["# Download data\n","!wget https://github.com/rdfia/rdfia.github.io/raw/master/data/3-b/imagenet_val_25.npz"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ps-orUXv-R3y"},"source":["f = np.load(\"imagenet_val_25.npz\", allow_pickle=True)\n","X, y, class_names = f[\"X\"], f[\"y\"], f[\"label_map\"].item() # class_names: id to name\n","class_names_to_id = {name: id for id, name in class_names.items()}\n","\n","plt.figure(figsize=(15, 7))\n","for i in range(24):\n","    plt.subplot(4, 6, i + 1)\n","    plt.imshow(X[i])\n","    plt.title(class_names[y[i]])\n","    plt.axis('off')\n","plt.gcf().tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ezmefelP-R35"},"source":["# Saliency Maps\n","\n","Calculate the saliency map for 5 examples out of the 25 loaded ones following the instructions of the TP guide.\n","\n","**Hint :** To choose 1 particular value in each row of a matrix, you can do this:\n","\n","```python\n","x = torch.Tensor([[0.1, 0.0, 0.5, 0.1, 0.1],\n","                  [0.0, 0.1, 0.0, 0.6, 0.2],\n","                  [0.7, 0.1, 0.1, 0.3, 0.0]])\n","x[np.arange(3), [2, 3, 0]]\n","# 0.5000\n","# 0.6000\n","# 0.7000\n","#[torch.FloatTensor of size 3]\n","```"]},{"cell_type":"code","metadata":{"id":"ucHO_isQ-R37"},"source":["def compute_saliency_maps(X, y, model):\n","    \"\"\"\n","    Compute a class saliency map using the model for images X and labels y.\n","\n","    Input:\n","    - X: Input images; Tensor of shape (N, 3, H, W)\n","    - y: Labels for X; LongTensor of shape (N,)\n","    - model: A pretrained CNN that will be used to compute the saliency map.\n","\n","    Returns:\n","    - saliency: A Tensor of shape (N, H, W) giving the saliency maps for the input\n","    images.\n","    \"\"\"\n","    # activate gradients on X\n","    X.requires_grad = True\n","    saliency = None\n","    # TODO\n","    output = model(X)\n","    logits = output[np.arange(len(y)), y].sum()\n","    logits.backward()\n","    saliency = X.grad.abs().max(dim=1)[0]\n","    # END TODO\n","\n","    return saliency"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBeE3JFR-R4C"},"source":["Test your code with the following function:"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"yHae91n0-R4E"},"source":["def show_saliency_maps(X, y, model):\n","    # Convert X and y from numpy arrays to Torch Tensors\n","    X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","    y_tensor = torch.LongTensor(y)\n","\n","    # Compute saliency maps for images in X\n","    saliency = compute_saliency_maps(X_tensor, y_tensor, model)\n","\n","    # Convert the saliency map from Torch Tensor to numpy array and show images\n","    # and saliency maps together.\n","    saliency = saliency.numpy()\n","    N = X.shape[0]\n","    for i in range(N):\n","        plt.subplot(2, N, i + 1)\n","        plt.imshow(X[i])\n","        plt.axis('off')\n","        plt.title(class_names[y[i]], fontsize=10)\n","        plt.subplot(2, N, N + i + 1)\n","        plt.imshow(saliency[i], cmap=plt.cm.hot)\n","        plt.axis('off')\n","        plt.gcf().set_size_inches(12, 5)\n","    plt.show()\n","\n","for i in range(1): # range(5) pour tester toutes les images\n","    show_saliency_maps(X[5*i:5*i+5], y[5*i:5*i+5], model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Extension: Test with VGG16**"],"metadata":{"id":"As5By0EGiq0K"}},{"cell_type":"code","source":["# Test with VGG16\n","model = torchvision.models.vgg16(weights='DEFAULT')\n","model.eval()\n","for param in model.parameters():\n","    param.requires_grad = False"],"metadata":{"id":"JKcV0K2McWaI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(1):\n","    show_saliency_maps(X[5*i:5*i+5], y[5*i:5*i+5], model)"],"metadata":{"id":"COGpA6u8cxTc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FOESl8Gj-R4L"},"source":["# Adversarial Examples (Fooling Images)\n","\n","Write the code to calculate an image such that it will be classified in a `target_y` different from the real class (by modifying the image and not the network parameters). See the TP guide for instructions.\n","\n","**The first two blocks will allow you to perform tests in an interactive way** to write and test your code. Once your code seems to work, complete the function in the 3rd block and test on various images in the 4th block."]},{"cell_type":"code","metadata":{"id":"rQNT2aP1-R4N"},"source":["# Initialize tests\n","X_tensor = torch.Tensor(preprocess(Image.fromarray(X[0])))\n","target_y = class_names_to_id['stingray']  # Desired class\n","X_fooling = X_tensor.clone()\n","X_fooling.requires_grad = True\n","learning_rate = 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model\n","model = torchvision.models.squeezenet1_1(weights='SqueezeNet1_1_Weights.DEFAULT')\n","\n","# Model in test mode\n","model.eval()\n","\n","# Freeze the weights\n","for param in model.parameters():\n","    param.requires_grad = False"],"metadata":{"id":"DZLmXDN6uLCI"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"b2hrwK0S-R4T"},"source":["# TODO\n","y_fooling = model(X_fooling).squeeze(0)[target_y]\n","y_fooling.backward()\n","with torch.no_grad():\n","  gradient = X_fooling.grad.clone()\n","  gradient /= gradient.norm()\n","  X_fooling += learning_rate*gradient\n","# END TODO"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize the image X_fooling and its modifications\n","plt.subplot(1, 2, 1)\n","plt.imshow(np.asarray(deprocess(X_fooling.clone())).astype(np.uint8))\n","plt.title(\"Image X_fooling\")\n","plt.axis('off')\n","plt.subplot(1, 2, 2)\n","plt.imshow(np.asarray(deprocess(10* (X_fooling - X_tensor), should_rescale=False)))\n","plt.title(\"Magnified difference with X_tensor (x10)\")\n","plt.axis('off')\n","plt.show()"],"metadata":{"id":"-9c5KNVXwROS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prediction with X_fooling\n","y_pred = model(X_fooling).squeeze(0).argmax().item()\n","print(\"Predicted class:\", class_names[y_pred])"],"metadata":{"id":"h6tpxjOXxO79"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sMWT0YVj-R4Z"},"source":["def make_fooling_image(X, target_y, model):\n","    \"\"\"\n","    Generate a fooling image that is close to X, but that the model classifies\n","    as target_y.\n","\n","    Inputs:\n","    - X: Input image; Tensor of shape (1, 3, 224, 224)\n","    - target_y: An integer in the range [0, 1000)\n","    - model: A pretrained CNN\n","\n","    Returns:\n","    - X_fooling: An image that is close to X, but that is classifed as target_y\n","    by the model.\n","    \"\"\"\n","    # Initialize our fooling image to the input image, enable gradients.\n","    X_fooling = X.clone()\n","    X_fooling.requires_grad = True\n","\n","    learning_rate = 1\n","    # TODO\n","    while True:\n","      if X_fooling.grad is not None:\n","            X_fooling.grad.zero_()\n","\n","      y_pred = model(X_fooling).squeeze(0)\n","      if y_pred.argmax().item() == target_y:\n","        break\n","      else:\n","        y_fooling = y_pred[target_y]\n","        y_fooling.backward()\n","        with torch.no_grad():\n","          gradient = X_fooling.grad.clone()\n","          gradient /= gradient.norm()\n","          X_fooling += learning_rate*gradient\n","    # END TODO\n","\n","    return X_fooling"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"LBRDXnEQ-R4g"},"source":["# Index of the image to modify and the target class\n","idx = 1\n","target_y = class_names_to_id['stingray']\n","\n","# Preparation of tensor X and it's \"fooling\" version\n","X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","X_fooling = make_fooling_image(X_tensor[idx:idx+1], target_y, model)\n","\n","# Check the predicted class\n","scores = model(X_fooling)\n","assert target_y == scores.data.max(1)[1][0], 'The model is not fooled!'\n","\n","# Display\n","X_fooling_np = deprocess(X_fooling.clone())\n","X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n","\n","plt.subplot(1, 4, 1)\n","plt.imshow(X[idx])\n","plt.title(class_names[y[idx]])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 2)\n","plt.imshow(X_fooling_np)\n","plt.title(class_names[target_y])\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 3)\n","X_pre = preprocess(Image.fromarray(X[idx]))\n","diff = np.asarray(deprocess(X_fooling - X_pre, should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Difference')\n","plt.axis('off')\n","\n","plt.subplot(1, 4, 4)\n","diff = np.asarray(deprocess(10 * (X_fooling - X_pre), should_rescale=False))\n","plt.imshow(diff)\n","plt.title('Magnified difference (10x)')\n","plt.axis('off')\n","\n","plt.gcf().set_size_inches(12, 5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7hXRsKrmiExt"},"source":["### **Extension: Test with different input images and different target classes**"]},{"cell_type":"code","source":["# Index of the image to modify and the target class\n","idx = [20, 22, 13] # daisy, modem, bee eater\n","target_y = [class_names_to_id['vase'], class_names_to_id['soap dispenser'], class_names_to_id['collie']]\n","\n","# Preparation of tensor X\n","X_tensor = torch.cat([preprocess(Image.fromarray(x)) for x in X], dim=0)\n","\n","for id, target in zip(idx, target_y):\n","  X_fooling = make_fooling_image(X_tensor[id:id+1], target, model)\n","\n","  # Check the predicted class\n","  scores = model(X_fooling)\n","  assert target == scores.data.max(1)[1][0], 'The model is not fooled!'\n","\n","  # Display\n","  X_fooling_np = deprocess(X_fooling.clone())\n","  X_fooling_np = np.asarray(X_fooling_np).astype(np.uint8)\n","\n","  plt.subplot(1, 4, 1)\n","  plt.imshow(X[id])\n","  plt.title(class_names[y[id]])\n","  plt.axis('off')\n","\n","  plt.subplot(1, 4, 2)\n","  plt.imshow(X_fooling_np)\n","  plt.title(class_names[target])\n","  plt.axis('off')\n","\n","  plt.subplot(1, 4, 3)\n","  X_pre = preprocess(Image.fromarray(X[id]))\n","  diff = np.asarray(deprocess(X_fooling - X_pre, should_rescale=False))\n","  plt.imshow(diff)\n","  plt.title('Difference')\n","  plt.axis('off')\n","\n","  plt.subplot(1, 4, 4)\n","  diff = np.asarray(deprocess(10 * (X_fooling - X_pre), should_rescale=False))\n","  plt.imshow(diff)\n","  plt.title('Magnified difference (10x)')\n","  plt.axis('off')\n","\n","  plt.gcf().set_size_inches(12, 5)\n","  plt.show()"],"metadata":{"id":"D6anABEr6GFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pArNLZ4p-R4k"},"source":["# Class visualization\n","\n","Write the code which generates an image maximizing the score of a class, subject to a certain number of regularizations. See the TP guide for details.\n"]},{"cell_type":"code","metadata":{"id":"H_7MJCB5-R4m"},"source":["def create_class_visualization(target_y, model, dtype, init_img=None, l2_reg=1e-3, learning_rate=5,\n","                               num_iterations=200, blur_every=10, max_jitter=16, show_every=25):\n","    \"\"\"\n","    Generate an image to maximize the score of target_y under a pretrained model.\n","\n","    Inputs:\n","    - target_y: Integer in the range [0, 1000) giving the index of the class\n","    - model: A pretrained CNN that will be used to generate the image\n","    - dtype: Torch datatype to use for computations\n","\n","    Keyword arguments:\n","    - init_img: Initial image to use (if None, will be random)\n","    - l2_reg: Strength of L2 regularization on the image\n","    - learning_rate: How big of a step to take\n","    - num_iterations: How many iterations to use\n","    - blur_every: How often to blur the image as an implicit regularizer\n","    - max_jitter: How much to jitter the image as an implicit regularizer\n","    - show_every: How often to show the intermediate result\n","    \"\"\"\n","    model.type(dtype)\n","\n","    # Randomly initialize the image as a PyTorch Tensor\n","    if init_img is None:\n","        img = torch.randn(1, 3, 224, 224).mul_(1.0).type(dtype).detach()\n","    else:\n","        img = init_img.clone().mul_(1.0).type(dtype).detach()\n","    img.requires_grad = True\n","\n","    for t in range(num_iterations):\n","        # Randomly jitter the image a bit; this gives slightly nicer results\n","        ox, oy = random.randint(0, max_jitter), random.randint(0, max_jitter)\n","        img = (jitter(img, ox, oy)).clone().detach()\n","        img.requires_grad = True\n","\n","        # TODO\n","        if img.grad is not None:\n","          img.grad.zero_()\n","\n","        y_pred = model(img).squeeze(0)\n","        logit = y_pred[target_y]\n","        L2_loss = logit - l2_reg*torch.norm(img)\n","        L2_loss.backward()\n","        with torch.no_grad():\n","          img += learning_rate*img.grad\n","        # END TODO\n","\n","        # Undo the random jitter\n","        img.data.copy_(jitter(img, -ox, -oy))\n","        img = img.clone()\n","\n","        # As regularizer, clamp and periodically blur the image\n","        for c in range(3):\n","            lo = float(-SQUEEZENET_MEAN[c] / SQUEEZENET_STD[c])\n","            hi = float((1.0 - SQUEEZENET_MEAN[c]) / SQUEEZENET_STD[c])\n","            img[:, c].clamp_(min=lo, max=hi)\n","        if t % blur_every == 0:\n","            blur_image(img, sigma=0.5)\n","\n","        # Periodically show the image\n","        if t == 0 or (t + 1) % show_every == 0 or t == num_iterations - 1:\n","            plt.imshow(deprocess(img.clone().cpu()))\n","            class_name = class_names[target_y]\n","            plt.title('%s\\nIteration %d / %d' % (class_name, t + 1, num_iterations))\n","            plt.gcf().set_size_inches(4, 4)\n","            plt.axis('off')\n","            plt.show()\n","\n","    return deprocess(img.cpu())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vgi-dnbf-R4s"},"source":["Test with various classes and starting from random noise:"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"nlgvIGSW-R4u"},"source":["dtype = torch.FloatTensor\n","# dtype = torch.cuda.FloatTensor # Uncomment this to use GPU\n","model.type(dtype)\n","\n","target_y = 76 # Tarantula\n","out = create_class_visualization(target_y, model, dtype, show_every=25, num_iterations=200)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Extension: Test for different numbers of iterations, learning rates and regularization weights**"],"metadata":{"id":"2w9WSGBCztWo"}},{"cell_type":"code","source":["num_iters = [100, 200, 300]\n","learning_rates = [1, 5, 10]\n","l2_regs = [1e-4, 1e-2, 2e0]\n","target_y = 76 # Tarantula\n","\n","for i in num_iters:\n","  print(f'__Number of iterations: {i}__')\n","  out = create_class_visualization(target_y, model, dtype, show_every=50, num_iterations=i)\n","for i in learning_rates:\n","  print(f'__Learning rate: {i}__')\n","  out = create_class_visualization(target_y, model, dtype, show_every=50, learning_rate=i)\n","for i in l2_regs:\n","  print(f'__L2 regularization weight: {i}__')\n","  out = create_class_visualization(target_y, model, dtype, show_every=50, l2_reg=i)"],"metadata":{"id":"strgcAO-zryv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w-ITjwDc-R4z"},"source":["### **Extension: Test by starting from an image from ImageNet**"]},{"cell_type":"code","metadata":{"id":"-BnCwzB1-R41"},"source":["img_ind = 20 # daisy\n","target_y = 985 # daisy\n","X_tensor = torch.Tensor(preprocess(Image.fromarray(X[img_ind])))\n","out = create_class_visualization(target_y, model, dtype, init_img=X_tensor, num_iterations=200)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Extension: Test with VGG16 network**"],"metadata":{"id":"j12LEfnq4zFc"}},{"cell_type":"code","source":["target_y = 76 # Tarantula\n","model_vgg = torchvision.models.vgg16(weights='DEFAULT')\n","dtype = torch.FloatTensor\n","model_vgg.type(dtype)\n","model_vgg.eval()\n","for param in model.parameters():\n","    param.requires_grad = False\n","out = create_class_visualization(target_y, model_vgg, dtype, num_iterations=200)"],"metadata":{"id":"09ShPcjw4ynb"},"execution_count":null,"outputs":[]}]}